###--------Problem Set 3 Big Data and Machine Learning------------###
###--------Daniela Jaime, Maria Paula Nieto, Juan Diego Lopez---------

##Paquetes 
rm(list = ls())

#install.packages("devtools")
require("pacman")
p_load("ggpubr")
p_load(faraway)
p_load(tidyverse)
p_load(skimr)
#devtools::install_github("boxuancui/DataExplorer")
p_load(DataExplorer)
p_load(scales)
p_load(corrr)
p_load(MASS)
p_load(class)
p_load(dplyr) #for data wrangling
p_load(gamlr)
p_load(dplyr)
p_load(glmnet)
p_load(pls)
#devtools::install_github("thomasp85/patchwork")
p_load(rvest)
p_load(rio) 
p_load(tidyverse)
p_load(e1071) 
p_load(EnvStats) 
p_load(tidymodels) 
p_load(ggplot2) 
p_load(scales) 
p_load(ggpubr) 
p_load(knitr) 
p_load(kableExtra)
p_load(broom)
p_load(caret)
p_load(rio,
       sf, # Leer/escribir/manipular datos espaciales
       leaflet, # Visualizaciones dinámicas
       tmaptools, # geocode_OSM()
       osmdata) # Get OSM's data
p_load(corrplot)

#Recuerden hacer el set de su directorio

#Cargar Datos

train <- readRDS("dataPS3/train.Rds")
test <- readRDS("dataPS3/test.Rds")

str(train)
summary(train)
summary(test)

##Se analizan los Nas
cantidad_na <- sapply(train, function(x) sum(is.na(x))) #Una función que me suma el número de NAs por variable
cantidad_na <- data.frame(cantidad_na) #Lo convierto en Data Frame
porcentaje_na <- cantidad_na/nrow(train) #Le saco el porcentaje de Missing values a cada variable

# Porcentaje de observaciones faltantes. 
porcentaje <- mean(porcentaje_na[,1]) #El 11.11% de las variables tiene NAs
print(paste0("En promedio el ", round(porcentaje*100, 2), "% de las entradas están vacías"))

##Ordenamos de mayor a menor
porcentaje_na <- arrange(porcentaje_na, desc(cantidad_na))
# Convertimos el nombre de la fila en columna
porcentaje_na <- rownames_to_column(porcentaje_na, "variable")

orden <- porcentaje_na$variable[length(porcentaje_na$variable):1] #Se vuelven caracteres
porcentaje_na$variable <- factor(porcentaje_na$variable,
                                 levels = orden) #Se utilizan como factores para poder graficar

ggplot(porcentaje_na[1:nrow(porcentaje_na),], 
       aes(y = variable, x = cantidad_na)) +
  geom_bar(stat = "identity", fill = "darkslategray3") +
  geom_text(aes(label = paste0(round(100*cantidad_na, 1), "%")),
            colour = "white", position = "dodge", hjust = 1.3,
            size = 2, fontface = "bold") +
  theme_classic() +
  labs(x = "Porcentaje de NAs", y = "Variables") +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1))

#Hay cuatro variables con muchos NAs: rooms, bathrooms, surface_total, surface_covered

#--------Combinar las bases-----------------
#Vamos a unir las bases test y train para hacer más fácil la transformación
test$test <- 1 #Binaria que me dice que este set es de test
train$test <- 0 #0 porque no son parte del test

test$price <- NA #Se crea la variable precio en Test como NAs para incluirla y poder combinar las bases

df <- rbind(train, test)

#Ahora es necesario volver el df un shape file
df_sf = st_as_sf(x=df,coords=c("lon","lat"),crs=4326)
class(df_sf)

table(df_sf$l3)

#Dvidir el set en Bogotá y Medellin 
df_sf_bog <- df_sf %>% filter(l3 == "Bogotá D.C")
df_sf_med <-  df_sf %>% filter(l3 == "Medellín")

#----------Visualización de los datos en el mapa-------------------

#Mirar los Mapas de cada ciudad
leaflet() %>% addTiles() %>% addCircleMarkers(data=df_sf_bog, color = "red", weight = 2)
leaflet() %>% addTiles() %>% addCircleMarkers(data=df_sf_med, color = "red", weight = 2)

#Este problem set se centra en Chapinero y El Poblado, por lo que se mirarán estas dos localidades

#Chapinero
chapinero <- getbb(place_name = "UPZ Chapinero, Bogota", 
                   featuretype = "boundary:administrative", 
                   format_out = "sf_polygon") %>% .$multipolygon

df_sf_chapinero <- st_crop (df_sf, chapinero)

leaflet() %>% addTiles() %>% addPolygons(data=chapinero) %>%
                            addCircleMarkers(data=df_sf_chapinero, color = "red", weight = 2)

#Poblado
poblado <- getbb(place_name = "Comuna 14 - El Poblado", 
                 featuretype = "boundary:administrative", 
                 format_out = "sf_polygon")

leaflet() %>% addTiles() %>% addPolygons(data=poblado)

df_sf_poblado <- st_crop (df_sf, poblado)

leaflet() %>% addTiles() %>% addPolygons(data=poblado) %>%
  addCircleMarkers(data=df_sf_poblado, color = "red", weight = 2)

#---------Limpieza y Analisis de Datos--------------------

#Análisis 
str(df_sf)
skim(df_sf)

#Con SKim nos damos cuenta que hay información que no es necesaria para los modelos
#Se eliminarán
df_sf <- df_sf %>% dplyr::select(-ad_type, -l1, -l2, -currency,
                                 -operation_type)

#Volver algunas variables como factores
df_sf <- df_sf %>%
  mutate_at(.vars = c("l3", "property_type"),
    .funs = factor)

#------Sacar los metros cuadrados en descripción-------
df_sf$surface_total[58] ## not surface_total

df_sf$surface_covered[58] ## not surface_covered

df_sf$description[58] ## explore description

#Como se observa en la observación analizada, para algunas observaciones sí se encuentra
#datos de la superficie cubierta pero no de la superficie total entonces se
#reemplaza la variable de superficie total por el máximo valor entre las dos

#Tenemos 88936 valores NAs de superficie total y 97371 na de superficie cubierta

df_sf$surface_total <- ifelse(is.na(df_sf$surface_total), df$surface_covered, df$surface_total)

##Ahora contamos con 79364 NAs


##Ahora bien, la variable de descripción cuenta con datos de utilidad como los M2
#por lo que se intentará extraer esta información con diferentes patrones 
##Se vuelven todas a minúsculas para mayor facilidad de lectura de patrones
df_sf$description <- tolower(df_sf$description)

##Se definen diferentes patrones
x = "[:space:]+[:digit:]+[:punct:]+[:digit:]+[:space:]+m2" ## pattern
y = "[:space:]+[:digit:]+[:space:]+metros+[:space:]+cuadrados"
z = "[:space:]+[:digit:]+[:space:]+metros"
w = "[:space:]+[:digit:]+[:space:]+área"
v = "[:space:]+[:digit:]+[:punct:]+[:digit:]+[:space:]+mt2"
a = "[:space:]+[:digit:]+[:space:]+mt2"
b = "[:space:]+[:digit:]+[:punct:]+[:digit:]+m2"
c = "[:space:]+[:digit:]+m2"
d = "[:space:]+[:digit:]+[:punct:]+[:digit:]+[:space:]+mts"
e = "[:space:]+[:digit:]+[:space:]+mts"
f = "[:space:]+[:digit:]+mts"
g = "[:space:]+[:digit:]+[:space:]+m2"

#Prueba con una observación:

str_locate_all(string = df_sf$description[14] , pattern = y) ## detect pattern

str_extract(string=df_sf$description[14] , pattern= y) ## extrac pattern

## Definir una nueva variable
df_sf = df_sf %>% 
  mutate(new_surface = str_extract(string=df_sf$description , pattern= y))
table(df_sf$new_surface)

## another pattern
df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= x),
                              new_surface))
table(df_sf$new_surface)

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= z),
                              new_surface))
table(df_sf$new_surface)

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= w),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= v),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= a),
                              new_surface))


df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= b),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= c),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= d),
                              new_surface))


df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= e),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= f),
                              new_surface))

df_sf = df_sf %>% 
  mutate(new_surface = ifelse(is.na(new_surface)==T,
                              str_extract(string=df_sf$description , pattern= g),
                              new_surface))
sum(is.na(df_sf$new_surface))

##72575 NAs

##Se hace lo mismo con pisos, estrato y parqueaderos
##Pisos

h = "[:space:]+[:digit:]+[:space:]+pisos" ## pattern
i = "[:space:]+[:digit:]+[:space:]+piso" 
j = "[:space:]+[:digit:]+pisos"


df_sf = df_sf %>% 
  mutate(pisos = str_extract(string=df_sf$description , pattern= h))
         
 df_sf = df_sf %>% 
  mutate(pisos = ifelse(is.na(pisos)==T,
                        str_extract(string=df_sf$description , pattern= i),
                        pisos))

df_sf = df_sf %>% 
  mutate(pisos = ifelse(is.na(pisos)==T,
                        str_extract(string=df_sf$description , pattern= j),
                        pisos))

sum(is.na(df_sf$pisos))
##110970 NAs


regexp <- "[[:digit:]]"

# process string
df_sf = df_sf %>% 
  mutate(pisos = str_extract(df_sf$pisos, regexp))

##Estrato

k = "[:space:]+estrato+[:space:]+[:digit:]" ## pattern
l = "[:space:]+estrato+[:digit:]"

df_sf = df_sf %>% 
  mutate(estrato = str_extract(string=df_sf$description , pattern= k))

df_sf = df_sf %>% 
  mutate(estrato = ifelse(is.na(estrato)==T,
                          str_extract(string=df_sf$description , pattern= l),
                          estrato))
sum(is.na(df_sf$estrato))
##109389 NAs

##Parqueaderos

m = "[:space:]+[:digit:]+[:space:]+parqueadero" ## pattern
n = "[:space:]+[:digit:]+[:space:]+parqueaderos" 
o = "[:space:]+[:digit:]+[:space:]+garaje"
p = "[:space:]+[:digit:]+[:space:]+garajes"
q = "[:space:]+[:digit:]+[:space:]+vehiculos"

df_sf = df_sf %>% 
  mutate(parqueaderos = str_extract(string=df_sf$description , pattern= m))

df_sf = df_sf %>% 
  mutate(parqueaderos = ifelse(is.na(parqueaderos)==T,
                               str_extract(string=df_sf$description , pattern= n),
                               parqueaderos))
df_sf = df_sf %>% 
  mutate(parqueaderos = ifelse(is.na(parqueaderos)==T,
                               str_extract(string=df_sf$description , pattern= o),
                               parqueaderos))
df_sf = df_sf %>% 
  mutate(parqueaderos = ifelse(is.na(parqueaderos)==T,
                               str_extract(string=df_sf$description , pattern= p),
                               parqueaderos))
df_sf = df_sf %>% 
  mutate(parqueaderos = ifelse(is.na(parqueaderos)==T,
                               str_extract(string=df_sf$description , pattern= q),
                               parqueaderos))
sum(is.na(df_sf$parqueaderos))
##97970 NAs

cantidad_na <- sapply(df_sf,function(x) sum(is.na(x))) #Una función que me suma el número de NAs por variable
cantidad_na <- data.frame(cantidad_na) #Lo convierto en Data Frame
porcentaje_na <- cantidad_na/nrow(df_sf)

porcentaje_na <- arrange(porcentaje_na, desc(cantidad_na))
# Convertimos el nombre de la fila en columna
porcentaje_na <- rownames_to_column(porcentaje_na, "variable")

orden <- porcentaje_na$variable[length(porcentaje_na$variable):1] #Se vuelven caracteres
porcentaje_na$variable <- factor(porcentaje_na$variable,
                                 levels = orden) #Se utilizan como factores para poder graficar


ggplot(porcentaje_na[1:nrow(porcentaje_na),], 
       aes(y = variable, x = cantidad_na)) +
  geom_bar(stat = "identity", fill = "darkslategray3") +
  geom_text(aes(label = paste0(round(100*cantidad_na, 1), "%")),
            colour = "white", position = "dodge", hjust = 1.3,
            size = 2, fontface = "bold") +
  theme_classic() +
  labs(x = "Porcentaje de NAs", y = "Variables") +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1))

##De las creadas la que tiene menos Nas es la de la superficie y los parqueaderos entonces se extraeran los 
##números de estas variables-

# prepare regular expression
regexp <- "[[:digit:]]"

# process string
df_sf = df_sf %>% 
  mutate(parqueaderos = str_extract(df_sf$parqueaderos, regexp))


regexp <- "[:digit:]+[:punct:]+[:digit:]"

# process string
df_sf = df_sf %>% 
  mutate(surface_total = ifelse(is.na(surface_total)==T,
                               str_extract(string=df_sf$new_surface , pattern= regexp),
                               surface_total))
regp <- "[[:digit:]]"
df_sf = df_sf %>% 
  mutate(surface_total = ifelse(is.na(surface_total)==T,
                                str_extract(string=df_sf$new_surface , pattern= regp),
                                surface_total))

sum(is.na(df_sf$surface_total))
##Los NAs se reducen significativamente a solo 46186.

df_sf = df_sf %>% 
  mutate(surface_total = as.numeric(gsub(",", ".", df_sf$surface_total)))

df_sf = df_sf %>% 
  mutate(surface_total = as.numeric(df_sf$surface_total))

##=== 2. vecinos espaciales para complementar observaciones ===##

#--------------------Manzanas Bogotá---------
# load data
mnzbog = st_read("MANZ.shp")
colnames(mnz)

leaflet() %>% addTiles() %>% addPolygons(data=mnzbog , color="red") %>% addCircles(data=df_sf)

## spatial join
sf::sf_use_s2(FALSE)
mnzbog <- st_transform(mnzbog, st_crs(df_sf))
df_sf_mnz = st_join(x = df_sf,y = mnzbog)

colnames(df_sf_mnz)

## average block
df_sf_mnz = df_sf_mnz %>%
  group_by(MANCODIGO) %>%
  mutate(new_surface_2=median(surface_total,na.rm=T))

table(is.na(df_sf_mnz$surface_total))

table(is.na(df_sf_mnz$surface_total),
      is.na(df_sf_mnz$new_surface_2)) # ahora solo tenemos 3083 missing values

df_sf = df_sf %>% 
  mutate(surface_total = ifelse(is.na(surface_total)==T,
                                as.numeric(df_sf_mnz$new_surface_2),
                                surface_total))
table(is.na(df_sf$surface_total))

##Ahora solo hay 3083 NAs de superficie total

####-------------Ahora diligenciamos información de estrato de Bogotá

estratobog = st_read("ManzanaEstratificacion.shp")
colnames(estratobog)

leaflet() %>% addTiles() %>% addPolygons(data=mnzbog , color="red") %>% addCircles(data=df_sf)

## spatial join
estratobog <- st_transform(estratobog, st_crs(df_sf))
df_sf_estrato = st_join(x = df_sf,y = estratobog)

colnames(df_sf_mnz)

## average block
df_sf_estrato = df_sf_estrato %>%
  group_by(CODIGO_MAN) %>%
  mutate(estrato2=median(ESTRATO,na.rm=T))

table(is.na(df_sf_estrato$estrato2))
##Tenemos 60128 missings

regexp <- "[[:digit:]]"

# process string
df_sf = df_sf %>% 
  mutate(estrato = str_extract(df_sf$estrato, regexp))

df_sf = df_sf %>% 
  mutate(estrato = ifelse(is.na(estrato)==T,
                    as.numeric(df_sf_estrato$estrato2),
                    estrato))
table(is.na(df_sf$estrato))
##Uniendo los datos con los extraídos de la descripción ahora se tienen 54971 NAs

#----------Manzanas Medellín--------------------

# load data
mnzmed = st_read("Manzanas_07-20.shp")
colnames(mnz)

leaflet() %>% addTiles() %>% addPolygons(data=mnzmed , color="red") %>% addCircles(data=df_sf)

## spatial join
sf::sf_use_s2(FALSE)
mnzmed <- st_transform(mnzmed, st_crs(df_sf))
df_sf_mnz = st_join(x = df_sf,y = mnzmed)

colnames(df_sf_mnz)

## average block
df_sf_mnz = df_sf_mnz %>%
  group_by(COBAMA) %>%
  mutate(new_surface_2=median(surface_total,na.rm=T))

table(is.na(df_sf_mnz$surface_total))

table(is.na(df_sf_mnz$surface_total),
      is.na(df_sf_mnz$new_surface_2)) # ahora solo tenemos 3083 missing values

df_sf = df_sf %>% 
  mutate(surface_total = ifelse(is.na(surface_total)==T,
                                as.numeric(df_sf_mnz$new_surface_2),
                                surface_total))
table(is.na(df_sf$surface_total))

##Ahora ya no tenemos NAs en la variable de superficie total

#------------------------Utilizar bases externas para crear otras variables

###Bares 

barbog = opq(bbox = st_bbox(df_sf)) %>%
  add_osm_feature(key = "amenity", value = "bar") %>%
  osmdata_sf() %>% .$osm_points 
barbog %>% head()

dist_bar = st_distance(x=df_sf , y=barbog)
dist_bar

min_dist = apply(dist_bar , 1 , min)
min_dist

df_sf = df_sf %>% 
  mutate(dist_bar = apply(dist_bar , 1 , min))

##Puntos de transporte 
bus_station = opq(bbox = st_bbox(df_sf)) %>%
  add_osm_feature(key = "amenity", value = "bus_station") %>%
  osmdata_sf() %>% .$osm_points 

bus_station %>% head()

dist_bus = st_distance(x=df_sf , y=bus_station)
dist_bus

df_sf = df_sf %>% 
  mutate(dist_bus = apply(dist_bus , 1 , min))
min_dist

##Ciclovia
ciclovia = st_read("Ciclovia.shp")
st_crs(ciclovia)
ciclovia<-st_transform(ciclovia, 4686)
ciclovia <- st_set_crs(ciclovia, 4686)

st_crs(df_sf_bog)
df_sf_bog<-st_transform(df_sf_bog, 4686)


dist_ciclovia = st_distance(df_sf_bog, ciclovia)

##Es necesario agregar las obsevaciones de Medellín a esta variable para pegarla a df_sf

ciclovia1 = st_read("geo_export_841eee89-8ede-4841-acaa-3668c6f31aab.shp")
ciclovia1<-st_transform(ciclovia1, 4686)
ciclovia1 <- st_set_crs(ciclovia1, 4686)
df_sf_med<-st_transform(df_sf_med, 4686)

myvars <- names(ciclovia1) %in% c("estado", "label", "nombre", "objectid", "shapelen")
ciclovia1 <- ciclovia1[!myvars]

dist_ciclovia1 = st_distance(df_sf_med, ciclovia1)

df_sf = df_sf %>% 
  mutate(ciclovia =NA)

df_sf = df_sf %>% 
  mutate(ciclovia = ifelse(l3=="Bogotá D.C", apply(dist_ciclovia , 1 , min), apply(dist_ciclovia1 , 1 , min))) 
         
##Parques 
parques = opq(bbox = st_bbox(df_sf)) %>%
  add_osm_feature(key = "leisure", value = "park") %>%
  osmdata_sf() %>% .$osm_points 
##Es muy pesado el vector para sacar la distancia entonces se saca por ciudades

parquesbog <- opq(bbox = getbb("Bogota Colombia")) %>%
  add_osm_feature(key = "leisure", value = "park")

parques_sf <- osmdata_sf(parquesbog)
parques_geometria <- parques_sf$osm_polygons 
parques_geometria <-st_transform(parques_geometria, 4686)
df_sf_chapinero <-st_transform(df_sf_chapinero, 4686)

dist_parkbog = st_distance(x=df_sf_chapinero , y=parques_geometria)
dist_parkbog

##Ahora Medellín
parquesmed <- opq(bbox = getbb("Medellin Colombia")) %>%
  add_osm_feature(key = "leisure", value = "park")

parques_sf <- osmdata_sf(parquesmed)
parques_geometria <- parques_sf$osm_polygons 
parques_geometria <-st_transform(parques_geometria, 4686)
df_sf_poblado <- st_transform(df_sf_poblado, 4686)

dist_parkmed = st_distance(x=df_sf_poblado, y=parques_geometria)
dist_parkmed

df_sf = df_sf %>% 
  mutate(parques =NA)

df_sf = df_sf %>% 
  mutate(parques = ifelse(l3=="Bogotá D.C", apply(dist_parkbog , 1 , min), apply(dist_parkmed , 1 , min))) 

#------------------Delitos de hurto a personas Bogotá

crimenbog = st_read("DAISCAT.shp")
colnames(crimenbog)
CMHPTOTAL

leaflet() %>% addTiles() %>% addPolygons(data=crimenbog , color="red") %>% addCircles(data=df_sf)

crimenbog <- st_transform(crimenbog, st_crs(df_sf_bog))
df_sf_bog= st_join(x = df_sf_bog, y = crimenbog)

df_sf_bog = df_sf_bog %>% 
  mutate(crimenbog =df_sf_bog$CMHP19CONT)

df_sf = df_sf %>% 
  mutate(crimen =NA)

df_sf = df_sf %>% 
  mutate(crimen = ifelse(l3=="Bogotá D.C", df_sf_bog$CMHP19CONT, NA))

df_sff <- df_sf
              
df_sff = data.frame(lapply(df_sff, as.character), stringsAsFactors=FALSE)  
saveRDS(df_sf, file = "df_sf.rds")

##-----------------Contaminación Bogotá 
contaminacionbog = st_read("Ozono_Prom_Anual.shp")
colnames(contaminacionbog)

leaflet() %>% addTiles() %>% addPolygons(data=contaminacionbog , color="red") %>% addCircles(data=df_sf_bog)
conc_ozono
contaminacionbog <-st_transform(contaminacionbog, 4686)

contaminacionbog <- st_transform(contaminacionbog, st_crs(df_sf_bog))
df_sf_contaminacion = st_join(x = df_sf_bog, y = contaminacionbog)

df_sf_bog = df_sf_bog %>% 
  mutate(contaminacion =df_sf_contaminacion$conc_ozono)

##Hay pocas observaciones en la base de contaminación por lo que si se va a analizar 
##solo chapinero no habría mucha variación en la variable para cada vivienda.

##---------------------Colegios de Bogotá

colegiosbog = st_read("Colegios_2022_03.shp")
colnames(colegiosbog)

leaflet() %>% addTiles() %>% addPolygons(data=colegiosbog , color="red") %>% addCircles(data=df_sf_bog)


colegiosbog <-st_transform(colegiosbog, 4686)

dist_colbog = st_distance(x=df_sf_bog, y=colegiosbog)
dist_colbog

df_sf = df_sf %>% 
  mutate(colegios =NA)

df_sf = df_sf %>% 
  mutate(colegios= ifelse(l3=="Bogotá D.C", apply(dist_colbog , 1 , min), NA)) 


##--------------------CAIs de Bogotá

CAIbog = st_read("ComandoAtencionInmediata.shp")
colnames(CAIbog)

leaflet() %>% addTiles() %>% addPolygons(data=CAIbog , color="red") %>% addCircles(data=df_sf_bog)

CAIbog <-st_transform(CAIbog, 4686)

dist_CAIbog = st_distance(x=df_sf_bog, y=CAIbog)
dist_CAIbog

df_sf = df_sf %>% 
  mutate(CAI =NA)

df_sf = df_sf %>% 
  mutate(CAI = ifelse(l3=="Bogotá D.C", apply(dist_colbog , 1 , min), NA))

#-------------------Centros comerciales Bogotá

Centroscomercialesbog = st_read("CCo.shp")
colnames(Centroscomercialesbog)


Centroscomercialesbog <-st_transform(Centroscomercialesbog, 4686)

dist_CCbog = st_distance(x=df_sf_bog, y=Centroscomercialesbog)
dist_CCbog


df_sf_bog = df_sf_bog %>% 
  mutate(centrocomercial= apply(dist_colbog , 1 , min))

df_sf = df_sf %>% 
  mutate(centrocomercial =NA)

df_sf = df_sf %>% 
  mutate(centrocomercial = ifelse(l3=="Bogotá D.C", df_sf_bog$centrocomercial, NA))

head(df_sf$centrocomercial)


#-------------------Colegios Medellín

colegiosmed = st_read("geo_export_5a7b5572-1445-488e-8738-7451a2951dc9.shp")
colnames(colegiosmed)

leaflet() %>% addTiles() %>% addPolygons(data=colegiosmed , color="red") %>% addCircles(data=df_sf_med)


colegiosmed <-st_transform(colegiosmed, 4686)

dist_colmed = st_distance(x=df_sf_med, y=colegiosmed)
dist_colmed

df_sf = df_sf %>% 
  mutate(colegios= ifelse(l3=="Bogotá D.C", apply(dist_colbog , 1 , min),  apply(dist_colmed , 1 , min))) 

##-----------------Hurtos a personas Medellín

crimenmed = read.csv("hurto_a_persona.csv", sep = ';')
coordinates(crimenmed)<-~longitud+latitud #
crimenmed <- str(crimenmed)


colnames(crimenmed)


leaflet() %>% addTiles() %>% addPolygons(data=crimenmed , color="red") %>% addCircles(data=df_sf_med)

crimenmed <- st_transform(crimenmed, st_crs(df_sf_med))
df_sf_med= st_join(x = df_sf_med, y = crimenmed)

df_sf_med = df_sf_med %>% 
  mutate(crimenmed =df_sf_med$CMHP19CONT)


df_sf = df_sf %>% 
  mutate(crimen = ifelse(l3=="Bogotá D.C", df_sf_bog$CMHP19CONT, df_sf_med$kj))



#-------------------Sacar la distancia a los business centers

#En primer lugar se define a la plaza de bolivar como CBC para Bogotá

CBC1bog <- geocode_OSM("Plaza de Bolívar, Bogotá", as.sf=T)
leaflet() %>% addTiles() %>% addCircles(data=CBC1bog)

CBC1bog <-st_transform(CBC1bog, 4686)

dist_CBCbog = st_distance(x=df_sf_bog , y=CBC1bog)


#Se define ahora la Plaza de Botero para Medellín
CBC1med <- geocode_OSM("Plaza de Botero, Medellin", as.sf=T)
leaflet() %>% addTiles() %>% addCircles(data=CBC1med)

CBC1med <-st_transform(CBC1med, 4686)

dist_CBCmed = st_distance(x=df_sf_med , y=CBC1med)


df_sf = df_sf %>% 
  mutate(distancia_CBC= ifelse(l3=="Bogotá D.C", apply(dist_CBCbog , 1 , min),  apply(dist_CBCmed , 1 , min))) 



#Correlaciones
MCor <- df %>% dplyr::select(rooms, bedrooms, bathrooms, surface_total,
                         surface_covered, price)

M <- cor(MCor, use = "pairwise.complete.obs")
corrplot(M, method = "ellipse", type = "full")

#Visualización de variables
ggplot(df_sf, aes(x = price)) + geom_histogram()
ggplot(df_sf, aes(x = price)) + geom_boxplot()

ggplot(df_sf, aes(x = rooms)) + geom_histogram()
ggplot(df_sf, aes(x = rooms)) + geom_boxplot()

ggplot(df_sf, aes(x = bedrooms)) + geom_histogram()
ggplot(df_sf, aes(x = bedrooms)) + geom_boxplot()

ggplot(df_sf, aes(x = surface_covered)) + geom_histogram()
ggplot(df_sf, aes(x = surface_covered)) + geom_boxplot()

#Variables categoricas y su relacion con el precio
ggplot(df_sf, aes(x = price)) + geom_boxplot() + facet_wrap(~ l3)

ggplot(df_sf, aes(x = price)) + geom_boxplot() + facet_wrap(~ property_type)

#Algunas combinaciones
ggplot(df_sf, aes(x = bathrooms, y= price, color = l3)) + geom_point()
ggplot(df_sf, aes(x = bedrooms, y= price, color = l3)) + geom_point()
ggplot(df_sf, aes(x = surface_covered, y= price, color = l3)) + geom_point()



#------Data Cleaning---------------


#------Dividir la muestra train dos submuestras-------------

#------------Modelos Continuos-----------------

require(caret)
require("gtsummary")

df_sf <- df_sf %>%
  mutate(estrato = as.numeric(estrato))
df_sf <- df_sf %>%
  mutate(parqueaderos = as.numeric(parqueaderos))

df_sf <- df_sf %>%
  mutate(pisos = as.numeric(pisos))

df_train <- subset(df_sf, test == 0)
df_train <- dplyr::select(df_train,-title, -property_id, -description, -new_surface, -contaminacion, -start_date, -end_date, -created_on, -geometry)
set.seed(1948)
df_train <-  st_drop_geometry(df_train)
index <-  round(nrow(df_train)*0.3,digits=0)
#Muestra aleatorizada del  dataset y mantener el número de observaciones del indice
test.indices <- sample(1:nrow(df_train), index)
# set de entrenamiento
dff_train<-df_train[-test.indices,] 
#30% set de testeo
dff_test<-df_train[test.indices,] 
#Seleccionar el set de entrenamiento en variables independientes y dependientes
YTrain <- dff_train$price
XTrain <- dff_train %>% dplyr::select(-price)
#Seleccionar el set de testeo en variables independientes y dependientes
YTest <- dff_test$price
XTest <- dff_test %>% dplyr::select(- price)


#--------Modelo XGboost---------------------
#Se crea la base a utilizar
YTestReg <- dff_test$price
dff_trainReg <- dff_train 
YTrainReg <- dff_trainReg$price
XTrainReg <- dff_trainReg %>% dplyr::select(-price)


set.seed(1948)
trControl1 <- trainControl(method='repeatedcv', 
                           number=5, 
                           repeats=1,
                           allowParallel = TRUE)


grid_default <- expand.grid(nrounds = c(250,500),
                            max_depth = c(4,5,6),
                            eta = c(0.05),
                            gamma = c(0.01),
                            min_child_weight = c(10, 25, 50),
                            colsample_bytree = c(0.7),
                            subsample = c(0.6))

ModeloXGBoost <- train(
  price ~ . ,
  data = dff_trainReg,
  method = "xgbTree",
  trControl = trControl1,
  tuneGrid = grid_default, na.action=na.exclude)


plot(ModeloXGBoost)
ModeloXGBoost
varImp(ModeloXGBoost,scale=TRUE)

levels(droplevels(XTest$parqueaderos))

YhatXG <-predict(ModeloXGBoost, XTest)
MSEXG <- sum((YTestReg-YhatXG)^2)/length(YTestReg)
RMSEXG <- sqrt(MSEXG)

MSEXGValor = 847419757645118080
RMSEGValor = 920554049.27

XTestXG <- cbind(XTest, YhatXG)
XTestXG$PricePredict <- ifelse(XTestXG$YhatXG > XTestXG$Npersug*XTestXG$Lp, 0, 1)
confusionMatrix(table(YTest, XTestXG$PricePredict))


#---------------Lasso y Ridge-----------------------------------------------------

#Seleccionamos la base y la modificamos. La variable dependiente sera price
#Se eliminan las variables que miden lo mismo 

df_train_price<- dplyr::select(dff_train,-l3)
df_test_price <- dff_test

#Se crean las variables de entrenamiento 
YTrain_price <- df_train_price$price
XTrain_price <- dplyr::select(df_train_price,-price)

#Se crean las variables de test 
YTest_price<-df_test_price$price
XTest_price<-dplyr::select(df_test_price,-price)

#Estimacion modelo OLS 
modelo_OLS_price<-lm(price~ ., data = df_train_price)
summary(modelo_OLS_price) #Adjuted R-squared 0.3113
#Coeficientes del modelo OLS 
mt_coeficientes_price <- modelo_OLS_price$coefficients %>%
  enframe(name = "predictor", value = "coeficiente")

mt_coeficientes_price %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo OLS Precio") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 5, angle = 45))
#Predicciones y MSE 
#Base de entrenamiento 
pred_OLS_train_price<- predict(modelo_OLS_price, newdata = df_train_price)
OLS_training_mse_price <- mean((pred_OLS_train_price - YTrain_price)^2)
paste("Error (mse) de entrenamiento:",OLS_training_mse_price)
#Base de prueba 
pred_OLS_test_price <- predict(modelo_OLS_price, newdata = df_test_price)
OLS_test_mse_price<-mean((pred_OLS_test_price - YTest_price)^2)
paste("Error (mse) de test:", OLS_test_mse_price)

#Regularizacion - Ridge 
require(glmnet)

#Se crean las matrices de entrenamiento y test 
x_train_ridge_price <- model.matrix(price~., data =df_train_price)[, -1]
y_train_ridge_price <- df_train_price$price

x_test_ridge_price <- model.matrix(price~., data = df_test_price)[, -1]
y_test_ridge_price <- df_test_price$price
#Creacion de modelo
#Para Ridge el alpha debe ser igual a 0 
modelo_ridge_price <- glmnet(
  x           = x_test_ridge_price,
  y           = y_test_ridge_price,
  alpha       = 0,
  nlambda     = 100,
  standardize = TRUE
)

#Cambio de coeficientes en funcion de lambda 
regularizacion_ridge_price <- modelo_ridge_price$beta %>% 
  as.matrix() %>%
  t() %>% 
  as_tibble() %>%
  mutate(lambda = modelo_ridge_price$lambda)

regularizacion_ridge_price<- regularizacion_ridge_price %>%
  pivot_longer(
    cols = !lambda, 
    names_to = "predictor",
    values_to = "coeficientes"
  )

regularizacion_ridge_price %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo en función de la regularización Precio") +
  theme_bw() +
  theme(legend.position = "none")

#MSE en funcion de lambda
set.seed(0711)
cv_error_ridge_price <- cv.glmnet(
  x      = x_train_ridge_price,
  y      = y_train_ridge_price,
  alpha  = 0,
  nfolds = 10,
  type.measure = "mse",
  standardize  = TRUE
)

plot(cv_error_ridge_price)

#Encontramos el lambda que minimiza MSE 
paste("Mejor valor de lambda encontrado:", cv_error_ridge_price$lambda.min)
#Mayor lambda que no se aleja más de 1 desviacion estandar del minimo 
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error_ridge_price$lambda.1se)

#Estimamos modelo usando mejor valor lambda mas una desviacion estandar 
modelo_lambda_ods_price <- glmnet(
  x           = x_train_ridge_price,
  y           = y_train_ridge_price,
  alpha       = 0,
  lambda      = cv_error_ridge_price$lambda.min,
  standardize = TRUE
)

#Analisis de coeficientes 
df_coeficientes_ridge <- coef(modelo_lambda_ods_price) %>%
  as.matrix() %>%
  as_tibble(rownames = "predictor") %>%
  rename(coeficiente = s0)

df_coeficientes_ridge %>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Ridge") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45))
#Predicciones Ridge Train 
pred_ridge_train_price <- predict(modelo_lambda_ods_price, newx = x_train_ridge_price)
#MSE train 
ridge_training_mse_price <- mean((pred_ridge_train_price - y_train_ridge_price)^2)
paste("Error (mse) de entrenamiento:", ridge_training_mse_price)
#Predicciones Ridge Test 
pred_ridge_test_price <- predict(modelo_lambda_ods_price, newx = x_test_ridge_price)
#MSE test 
test_mse_ridge_price <- mean((pred_ridge_test_price - y_test_ridge_price)^2)
paste("Error (mse) de test:", test_mse_ridge_price)

#Regularizacion - Lasso 
#Matrices de entrenamiento y test
x_train_lasso_price <- model.matrix(price~., data = df_train_price)[, -1]
y_train_lasso_price <- df_train_price$price

x_test_lasso_price <- model.matrix(price~., data = df_test_price)[, -1]
y_test_lasso_price <- df_test_price$price

#Creacion del modelo 
modelo_price_lasso <- glmnet(
  x           = x_train_lasso_price,
  y           = y_train_lasso_price,
  alpha       = 1,
  nlambda     = 100,
  standardize = TRUE
)

#Coeficientes en funcion de lambda 
regularizacion_lasso_price <- modelo_price_lasso$beta %>% 
  as.matrix() %>%
  t() %>% 
  as_tibble() %>%
  mutate(lambda = modelo_price_lasso$lambda)

regularizacion_lasso_price <- regularizacion_lasso_price %>%
  pivot_longer(
    cols = !lambda, 
    names_to = "predictor",
    values_to = "coeficientes"
  )
regularizacion_lasso_price %>%
  ggplot(aes(x = lambda, y = coeficientes, color = predictor)) +
  geom_line() +
  scale_x_log10(
    breaks = trans_breaks("log10", function(x) 10^x),
    labels = trans_format("log10", math_format(10^.x))
  ) +
  labs(title = "Coeficientes del modelo en función de la regularización Precio") +
  theme_bw() +
  theme(legend.position = "none")
#MSE en funcion de lambda 
set.seed(0711)
cv_error_lasso_price <- cv.glmnet(
  x      = x_train_lasso_price,
  y      = y_train_lasso_price,
  alpha  = 1,
  nfolds = 10,
  type.measure = "mse",
  standardize  = TRUE
)

plot(cv_error_lasso_price)

#Mejor lambda determinado 
paste("Mejor valor de lambda encontrado:", cv_error_lasso_price$lambda.min)

#Mejor lambda a una desviacion estandar 
paste("Mejor valor de lambda encontrado + 1 desviación estándar:", cv_error_lasso_price$lambda.1se)

#Estimacion de modelo con lambda optimo 
modelo_op_lasso_price <- glmnet(
  x           = x_train_lasso_price,
  y           = y_train_lasso_price,
  alpha       = 1,
  lambda      = cv_error_lasso_price$lambda.min,
  standardize = TRUE
)

#Coeficientes 
df_coef_lasso_price<- coef(modelo_op_lasso_price) %>%
  as.matrix() %>%
  as_tibble(rownames = "predictor") %>%
  rename(coeficiente = s0)

df_coef_lasso_price%>%
  filter(predictor != "(Intercept)") %>%
  ggplot(aes(x = predictor, y = coeficiente)) +
  geom_col() +
  labs(title = "Coeficientes del modelo Lasso") +
  theme_bw() +
  theme(axis.text.x = element_text(size = 6, angle = 45))

df_coef_lasso_price %>%
  filter(
    predictor != "(Intercept)",
    coeficiente != 0
  ) 

#Predicciones Ridge Train 
pred_lasso_train_price <- predict(modelo_op_lasso_price
                                , newx = x_train_lasso_price)
#MSE train 
lasso_training_mse_price <- mean((pred_lasso_train_price - y_train_lasso_price)^2)
paste("Error (mse) de entrenamiento:", lasso_training_mse_price)
#Predicciones Ridge Test 
pred_lasso_test_price <- predict(modelo_op_lasso_price, newx = x_test_lasso_price)
#MSE test 
lasso_test_mse_price <- mean((pred_lasso_test_price - y_test_lasso_price)^2)
paste("Error (mse) de test:", lasso_test_mse_price)

#-----------------Resultados preliminares regresion----------------------------------
#Se puede ver que en los modelos XGboost y Lasso hay un mayor peso de algunas variables.
#En el caso de XGBoost las variables más importantes son Arriendo (que se crea apartir de P5130 y P5140,
#Si no cotiza a pensión, el número de cuartos, la edad del jefe del hogar, si recibe ingresos por arriendo o pension,
#La educación del jefe del hogar, el tipo de vivienda que viven, y el tiempo que lleva en la empresa)
#Con esto en mente se crearán nuevos modelos para ser más precisos.
#Por otro lado, se utilizará la variable ingtotugarr porque vimos que tiene una relación muy grande con la variable
# arriendo y además es la que se usa para medir la linea de pobreza de los hogares

MSEValores <- c(OLS_test_mse_price, test_mse_ridge_price, lasso_test_mse_price, MSEXG)
RMSEValores <- sapply(MSEValores, sqrt)
ModeloNombre <- c("Modelo OLS", "Modelo Ridge", "Modelo Lasso", "Modelo XGBoost")

ResultadosInicialesRegresio <- data.frame(ModeloNombre, MSEValores, RMSEValores)

#----------Modelo 5 Elastic Net-------
el <- train(
  price ~ rooms + bedrooms + bathrooms + surface_total + estrato + parqueaderos + pisos + dist_bar + dist_bus + ciclovia + parques + crimen + colegios + CAI + centrocomercial +distancia_CBC , data = dff_train, method = "glmnet",
  trControl = trainControl("cv", number = 10), preProcess = c("center", "scale"))

plot(el)
el

YhatEl <- predict(el, dff_test)
MSEtest_el <- mean((YTest_ing - YhatEl)^2)
paste("Error (mse) de test:", MSEtest_el)
RMSE_el <- sqrt(MSEtest_el)

ModeloElStats <- c("Elastic Net", MSEtest_el, RMSE_el)

ResultadosInicialesRegresio <- rbind(ResultadosInicialesRegresio, ModeloElStats)


#-----------------Modelo 6------------------
ModeloPv <- lm(Ingtotugarr ~ Arriendo + PensionJefe_2 + JefeMujer + EdadJefe + as.factor(P5090) + HorasTrabJefe, 
               data = dff_train)

summary(ModeloPv)

YModeloPv <- predict(ModeloPv, dff_test)
MSEtestModeloPV <- mean((YTest_price - YModeloPv)^2)
RMSE_Pv <- sqrt(MSEtestModeloPV)
ModeloPvStats <- c("Modelo Pv", MSEtestModeloPV, RMSE_Pv)

ResultadosInicialesRegresio <- rbind(ResultadosInicialesRegresio, ModeloPvStats)

#------Modelo Ridge 7-----------

set.seed(1948)

lambda = 10^seq(-2, 3, length = 10)
ridge_caret<- train(Ingtotugarr ~ JefeMujer + EdadJefe + EducacionJefe_2 + EducacionJefe_3 + EducacionJefe_4 + EducacionJefe_5 + EducacionJefe_6 + ActividadJefe_2 + ActividadJefe_3 + ActividadJefe_4 + ActividadJefe_5 + ActividadJefe_6,
                    data = dff_train, 
                    method = "glmnet",
                    trControl = trainControl("cv", number = 10),
                    preProcess = c("center","scale"),
                    tuneGrid = expand.grid(alpha = 0,
                                           lambda = lambda))

plot(ridge_caret)
ridge_caret
summary(ridge_caret)

Yridge_caret <- predict(ridge_caret, dff_test)
MSEtestridge_caret <- mean((YTest_price - Yridge_caret)^2)
RMSE_rc <- sqrt(MSEtestridge_caret)
ridge_caretStats <- c("Modelo RidgeEducacionYActividad", MSEtestridge_caret, RMSE_rc)

ResultadosInicialesRegresio <- rbind(ResultadosInicialesRegresio, ridge_caretStats)
write.csv(ResultadosInicialesRegresio,"ResultadosDeModelos.csv", row.names = FALSE)



####OJO SI EL PRECIO ESTÁ MENOR EN 40 MILLONES O 10000 USD TENDREMOS PENALIZACIÓN #####


